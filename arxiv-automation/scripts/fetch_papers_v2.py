#!/usr/bin/env python3
"""
arXiv Papers Fetcher V2 (Enhanced with AI Analysis)
åœ¨ V1 åŸºç¡€ä¸Šå¢åŠ  PDF å…¨æ–‡ä¸‹è½½å’Œ AI æ·±åº¦åˆ†æåŠŸèƒ½
"""

import os
import re
import json
import time
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path
from openai import OpenAI
import tempfile
import subprocess

# é…ç½®
CATEGORIES = ["cs.CV", "cs.AI", "cs.MM", "cs.RO", "cs.LG"]
DAYS_TO_CHECK = 3
DAYS_TO_COMPARE = 5
VIDEO_KEYWORDS = [
    "video generation", "video synthesis", "video editing", "video edit",
    "video diffusion", "text-to-video", "image-to-video", "video-to-video",
    "video understanding", "video model", "temporal", "motion generation",
    "video quality", "video enhancement", "video restoration", "video prediction"
]

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
# ä¼˜å…ˆä½¿ç”¨ DEEPSEEK_API_KEYï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°é»˜è®¤é…ç½®
import os
api_key = os.environ.get("DEEPSEEK_API_KEY") or os.environ.get("OPENAI_API_KEY")
base_url = "https://api.deepseek.com" if os.environ.get("DEEPSEEK_API_KEY") else None
model_name = "deepseek-chat" if os.environ.get("DEEPSEEK_API_KEY") else "gemini-2.5-flash"

client = OpenAI(api_key=api_key, base_url=base_url)

def fetch_arxiv_papers(category, days=3, max_retries=3):
    """ä» arXiv API è·å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡"""
    base_url = "http://export.arxiv.org/api/query?"
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    query = f"cat:{category}"
    params = {
        "search_query": query,
        "start": 0,
        "max_results": 300,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    
    url = base_url + urllib.parse.urlencode(params)
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # arXiv å»ºè®®è¯·æ±‚é—´éš”è‡³å°‘ 3 ç§’
            if attempt > 0:
                wait_time = 5 * attempt
                print(f"  ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            
            with urllib.request.urlopen(url, timeout=30) as response:
                data = response.read()
            
            root = ET.fromstring(data)
            namespace = {"atom": "http://www.w3.org/2005/Atom"}
            
            papers = []
            for entry in root.findall("atom:entry", namespace):
                published = entry.find("atom:published", namespace).text
                pub_date = datetime.strptime(published, "%Y-%m-%dT%H:%M:%SZ")
                
                if pub_date < start_date:
                    continue
                
                paper = {
                    "id": entry.find("atom:id", namespace).text.split("/abs/")[-1],
                    "title": entry.find("atom:title", namespace).text.strip().replace("\n", " "),
                    "summary": entry.find("atom:summary", namespace).text.strip().replace("\n", " "),
                    "authors": [author.find("atom:name", namespace).text for author in entry.findall("atom:author", namespace)],
                    "published": pub_date.strftime("%Y-%m-%d"),
                    "pdf_url": entry.find("atom:id", namespace).text.replace("/abs/", "/pdf/"),
                    "abs_url": entry.find("atom:id", namespace).text,
                    "categories": [cat.attrib["term"] for cat in entry.findall("atom:category", namespace)]
                }
                
                papers.append(paper)
            
            return papers
        
        except Exception as e:
            print(f"  å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
            if attempt == max_retries - 1:
                print(f"Error fetching papers for {category}: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return []
            continue
    
    return []

def is_video_related(paper):
    """åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸è§†é¢‘ç›¸å…³"""
    text = (paper["title"] + " " + paper["summary"]).lower()
    return any(keyword.lower() in text for keyword in VIDEO_KEYWORDS)

def extract_links(paper):
    """ä»æ‘˜è¦ä¸­æå–é¡¹ç›®å’Œä»£ç é“¾æ¥"""
    text = paper["summary"]
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    urls = re.findall(url_pattern, text)
    
    links = {"project": None, "code": None}
    
    for url in urls:
        url_lower = url.lower()
        if "github.com" in url_lower or "gitlab.com" in url_lower:
            if not links["code"]:
                links["code"] = url
        elif "project" in url_lower or "page" in url_lower or "site" in url_lower:
            if not links["project"]:
                links["project"] = url
        elif not links["project"]:
            links["project"] = url
    
    return links

def download_pdf(pdf_url, output_path):
    """ä¸‹è½½ PDF æ–‡ä»¶"""
    try:
        print(f"  ä¸‹è½½ PDF: {pdf_url}")
        urllib.request.urlretrieve(pdf_url, output_path)
        return True
    except Exception as e:
        print(f"  PDF ä¸‹è½½å¤±è´¥: {e}")
        return False

def extract_text_from_pdf(pdf_path):
    """ä» PDF æå–æ–‡æœ¬"""
    try:
        # ä½¿ç”¨ pdftotext å·¥å…·
        result = subprocess.run(
            ["pdftotext", "-layout", pdf_path, "-"],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            return result.stdout
        else:
            print(f"  PDF æ–‡æœ¬æå–å¤±è´¥: {result.stderr}")
            return None
    
    except Exception as e:
        print(f"  PDF æ–‡æœ¬æå–é”™è¯¯: {e}")
        return None

def analyze_paper_with_ai(paper, pdf_text):
    """ä½¿ç”¨ AI åˆ†æè®ºæ–‡å…¨æ–‡"""
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆé¿å…è¶…è¿‡ token é™åˆ¶ï¼‰
    max_chars = 30000
    if len(pdf_text) > max_chars:
        # å–å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†
        half = max_chars // 2
        pdf_text = pdf_text[:half] + "\n\n[... ä¸­é—´éƒ¨åˆ†çœç•¥ ...]\n\n" + pdf_text[-half:]
    
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸“å®¶ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹è®ºæ–‡çš„å…¨æ–‡å†…å®¹ï¼Œå¹¶è¿›è¡Œæ·±åº¦åˆ†æã€‚

è®ºæ–‡æ ‡é¢˜: {paper['title']}

è®ºæ–‡å…¨æ–‡:
{pdf_text}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æï¼š

1. **æ ¸å¿ƒè§‚ç‚¹**: ç”¨æœ€ç®€å•ç›´ç™½çš„è¯­è¨€ï¼ˆ1-2å¥è¯ï¼‰è¯´æ˜è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹
2. **æŠ€æœ¯æ–¹æ³•**: ç®€è¦è¯´æ˜é‡‡ç”¨çš„ä¸»è¦æŠ€æœ¯æ–¹æ³•å’Œæ¶æ„
3. **å®éªŒéªŒè¯**: è¯„ä¼°å®éªŒè®¾è®¡çš„åˆç†æ€§ã€æ•°æ®é›†é€‰æ‹©ã€å¯¹æ¯”æ–¹æ³•æ˜¯å¦å……åˆ†
4. **ç»“æœå¯é æ€§**: åˆ†æå®éªŒç»“æœçš„å¯ä¿¡åº¦ï¼Œæ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆã€cherry-picking ç­‰é—®é¢˜
5. **å®ç”¨ä»·å€¼**: è¯„ä¼°è¯¥ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼å’Œå±€é™æ€§
6. **æ‰¹åˆ¤æ€§è¯„ä»·**: æŒ‡å‡ºè®ºæ–‡çš„ä¼˜ç‚¹å’Œä¸è¶³ï¼Œä»¥åŠå¯èƒ½å­˜åœ¨çš„é—®é¢˜

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ï¼Œæ¯ä¸ªæ–¹é¢æ§åˆ¶åœ¨ 2-3 å¥è¯ä»¥å†…ã€‚"""

    try:
        print(f"  AI åˆ†æä¸­...")
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ï¼Œæ“…é•¿æ‰¹åˆ¤æ€§é˜…è¯»å’Œè¯„ä¼°è®ºæ–‡è´¨é‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=2000
        )
        
        analysis = response.choices[0].message.content.strip()
        return analysis
    
    except Exception as e:
        print(f"  AI åˆ†æå¤±è´¥: {e}")
        return "AI åˆ†ææš‚æ—¶ä¸å¯ç”¨"

def translate_text(text, target_lang="zh"):
    """ä½¿ç”¨ AI ç¿»è¯‘æ–‡æœ¬"""
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are a professional translator. Translate the following academic text to Chinese. Keep technical terms in English when appropriate. Only return the translation, no explanations."},
                {"role": "user", "content": text}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Translation error: {e}")
        return text

def load_recent_papers(days=5):
    """åŠ è½½æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ ID ç”¨äºå»é‡"""
    # è‡ªåŠ¨è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ä¸‹çš„ papers æ–‡ä»¶å¤¹
    papers_dir = Path(__file__).parent.parent / "papers"
    recent_ids = set()
    
    if not papers_dir.exists():
        return recent_ids
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    for md_file in papers_dir.glob("*.md"):
        try:
            file_date = datetime.strptime(md_file.stem, "%Y-%m-%d")
            if file_date >= start_date:
                content = md_file.read_text(encoding="utf-8")
                ids = re.findall(r'arxiv\.org/abs/(\d+\.\d+)', content)
                recent_ids.update(ids)
        except:
            continue
    
    return recent_ids

def generate_markdown_v2(papers, date_str):
    """ç”Ÿæˆ V2 ç‰ˆæœ¬çš„ Markdownï¼ˆåŒ…å« AI åˆ†æï¼‰"""
    md_content = f"# arXiv Papers - {date_str} (V2 Enhanced)\n\n"
    md_content += f"**æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    md_content += f"**è®ºæ–‡æ•°é‡**: {len(papers)}\n\n"
    md_content += f"**ç‰ˆæœ¬**: V2 (åŒ…å« AI å…¨æ–‡åˆ†æ)\n\n"
    md_content += "---\n\n"
    
    for i, paper in enumerate(papers, 1):
        print(f"\nå¤„ç†è®ºæ–‡ {i}/{len(papers)}: {paper['id']}")
        
        # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦
        print(f"  ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦...")
        title_zh = translate_text(paper["title"])
        summary_zh = translate_text(paper["summary"])
        
        # æå–é“¾æ¥
        links = extract_links(paper)
        
        # ä¸‹è½½å¹¶åˆ†æ PDF
        ai_analysis = "PDF ä¸‹è½½æˆ–åˆ†æå¤±è´¥"
        with tempfile.TemporaryDirectory() as tmpdir:
            pdf_path = Path(tmpdir) / f"{paper['id']}.pdf"
            
            if download_pdf(paper['pdf_url'], pdf_path):
                pdf_text = extract_text_from_pdf(pdf_path)
                
                if pdf_text and len(pdf_text.strip()) > 500:
                    ai_analysis = analyze_paper_with_ai(paper, pdf_text)
                else:
                    ai_analysis = "PDF æ–‡æœ¬æå–å¤±è´¥æˆ–å†…å®¹è¿‡çŸ­"
        
        # ç”Ÿæˆ Markdown
        md_content += f"## {i}. {paper['title']}\n\n"
        md_content += f"**ä¸­æ–‡æ ‡é¢˜**: {title_zh}\n\n"
        md_content += f"**ä½œè€…**: {', '.join(paper['authors'][:5])}"
        if len(paper['authors']) > 5:
            md_content += f" et al."
        md_content += "\n\n"
        md_content += f"**å‘å¸ƒæ—¥æœŸ**: {paper['published']}\n\n"
        md_content += f"**arXiv ID**: [{paper['id']}]({paper['abs_url']})\n\n"
        md_content += f"**PDF**: [ä¸‹è½½é“¾æ¥]({paper['pdf_url']})\n\n"
        
        if links["project"]:
            md_content += f"**é¡¹ç›®ä¸»é¡µ**: {links['project']}\n\n"
        if links["code"]:
            md_content += f"**ä»£ç ä»“åº“**: {links['code']}\n\n"
        
        md_content += f"**ç±»åˆ«**: {', '.join(paper['categories'])}\n\n"
        
        # è‹±æ–‡æ‘˜è¦ï¼ˆæŠ˜å ï¼‰
        md_content += "<details>\n"
        md_content += "<summary><b>æ‘˜è¦ (Abstract)</b></summary>\n\n"
        md_content += f"{paper['summary']}\n\n"
        md_content += "</details>\n\n"
        
        # ä¸­æ–‡æ‘˜è¦ï¼ˆæŠ˜å ï¼‰
        md_content += "<details>\n"
        md_content += "<summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>\n\n"
        md_content += f"{summary_zh}\n\n"
        md_content += "</details>\n\n"
        
        # AI æ·±åº¦åˆ†æï¼ˆæŠ˜å ï¼‰
        md_content += "<details>\n"
        md_content += "<summary><b>ğŸ¤– AI é˜…è¯»åˆ†æ</b></summary>\n\n"
        md_content += f"{ai_analysis}\n\n"
        md_content += "</details>\n\n"
        
        md_content += "---\n\n"
        
        # é¿å… API é™æµ
        time.sleep(2)
    
    return md_content

def update_readme_index():
    """æ›´æ–° README ä¸­çš„è®ºæ–‡ç´¢å¼•"""
    base_dir = Path(__file__).parent.parent
    papers_dir = base_dir / "papers"
    readme_path = base_dir / "README.md"
    
    if not papers_dir.exists():
        return
    
    paper_files = sorted(papers_dir.glob("*.md"), reverse=True)
    
    index_content = "\n"
    for paper_file in paper_files:
        date_str = paper_file.stem
        content = paper_file.read_text(encoding="utf-8")
        count_match = re.search(r'\*\*è®ºæ–‡æ•°é‡\*\*: (\d+)', content)
        count = count_match.group(1) if count_match else "0"
        
        # æ£€æµ‹ç‰ˆæœ¬
        version = "V2" if "V2" in content else "V1"
        
        index_content += f"- [{date_str}](papers/{paper_file.name}) - {count} ç¯‡è®ºæ–‡ ({version})\n"
    
    readme_content = readme_path.read_text(encoding="utf-8")
    pattern = r'<!-- PAPERS_INDEX_START -->.*?<!-- PAPERS_INDEX_END -->'
    replacement = f'<!-- PAPERS_INDEX_START -->{index_content}<!-- PAPERS_INDEX_END -->'
    updated_content = re.sub(pattern, replacement, readme_content, flags=re.DOTALL)
    
    readme_path.write_text(updated_content, encoding="utf-8")

def main():
    print("=" * 60)
    print("arXiv Papers Fetcher V2 (Enhanced)")
    print("=" * 60)
    
    # åŠ è½½æœ€è¿‘çš„è®ºæ–‡ ID
    print(f"\nåŠ è½½æœ€è¿‘ {DAYS_TO_COMPARE} å¤©çš„è®ºæ–‡ ID...")
    recent_ids = load_recent_papers(DAYS_TO_COMPARE)
    print(f"å·²åŠ è½½ {len(recent_ids)} ä¸ªè®ºæ–‡ ID")
    
    # è·å–æ‰€æœ‰ç±»åˆ«çš„è®ºæ–‡
    all_papers = []
    for i, category in enumerate(CATEGORIES):
        print(f"\næ­£åœ¨è·å– {category} ç±»åˆ«çš„è®ºæ–‡...")
        papers = fetch_arxiv_papers(category, DAYS_TO_CHECK)
        print(f"è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        all_papers.extend(papers)
        
        # åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆarXiv å»ºè®®è‡³å°‘ 3 ç§’ï¼‰
        if i < len(CATEGORIES) - 1:
            print("  ç­‰å¾… 3 ç§’...")
            time.sleep(3)
    
    # å»é‡
    unique_papers = {}
    for paper in all_papers:
        if paper["id"] not in unique_papers:
            unique_papers[paper["id"]] = paper
    
    print(f"\nå»é‡åå…± {len(unique_papers)} ç¯‡è®ºæ–‡")
    
    # ç­›é€‰è§†é¢‘ç›¸å…³è®ºæ–‡
    video_papers = [p for p in unique_papers.values() if is_video_related(p)]
    print(f"ç­›é€‰å‡º {len(video_papers)} ç¯‡è§†é¢‘ç›¸å…³è®ºæ–‡")
    
    # æ’é™¤å·²æ”¶å½•è®ºæ–‡
    new_papers = [p for p in video_papers if p["id"] not in recent_ids]
    print(f"æ’é™¤å·²æ”¶å½•è®ºæ–‡åï¼Œå‰©ä½™ {len(new_papers)} ç¯‡æ–°è®ºæ–‡")
    
    if len(new_papers) == 0:
        print("\næ²¡æœ‰æ–°è®ºæ–‡éœ€è¦æ›´æ–°")
        return
    
    # æŒ‰å‘å¸ƒæ—¥æœŸæ’åº
    new_papers.sort(key=lambda x: x["published"], reverse=True)
    
    # ç”Ÿæˆ Markdown (V2 ç‰ˆæœ¬)
    date_str = datetime.now().strftime("%Y-%m-%d")
    print(f"\nç”Ÿæˆ V2 ç‰ˆæœ¬ Markdown æ–‡ä»¶: {date_str}.md")
    md_content = generate_markdown_v2(new_papers, date_str)
    
    # ä¿å­˜æ–‡ä»¶
    papers_dir = Path(__file__).parent.parent / "papers"
    papers_dir.mkdir(parents=True, exist_ok=True)
    output_file = papers_dir / f"{date_str}.md"
    output_file.write_text(md_content, encoding="utf-8")
    print(f"å·²ä¿å­˜åˆ°: {output_file}")
    
    # æ›´æ–° README ç´¢å¼•
    print("\næ›´æ–° README ç´¢å¼•...")
    update_readme_index()
    
    print("\n" + "=" * 60)
    print("å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
